import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 예제 데이터 생성
data = pd.DataFrame({
    'Feature1': [#유동인구
456.26,1034.91,470.59,379.15,744.36,551.88,489.58,609.09,343.32,439.41,364.84,445.94,802.45,740.63,575.98,485.57,1034.91,590.79,415.31,397.74,447.63,336.61,586.89,471.98,616.83
],
    'Feature2': [#교통량
46314,163223,57114,41930,95753,65780,75942,92783,74050,77138,51434,59928,91273,156528,72855,49405,129712,83159,99476,60894,44181,54913,73789,71119,111342
],
    'Feature3': [#도로율
        22.2,28.3,22.8,26.7,29.4,23.8,25.3,26.7,27.5,22.9,23,27.4,24.8,30.8,27,24.4,29.7,27.6,24.3,25,21.3,22.7,25.8,26.4,25.3

],
    'Feature4': [#녹지비율
        7.1,11.1,21.2,25.8,17.7,14.2,14.1,18.8,13.8,26.3,23.4,18.4,19.6,21.2,13.5,26.4,28.2,13.5,8.2,26.7,10.9,20.2,14.9,15.5,13.6

],
    'Feature5': [#건물수
        10979,60328,33530,37517,50434,34934,30257,39478,26185,43628,26901,30394,38244,47639,24685,38287,59636,35689,34504,32271,13208,30593,25526,27095,38220

],
    'Feature6': [ #건물 밀도
        1110,1527,1393,1563,1225,1455,1782,1484,1565,1246,1264,1598,1664,1013,1667,1556,1769,1928,1494,1076,574,1545,1734,1559,1734
],
    'Feature7': [ #건물 수/ 천 명
        7.76,11.43,8.84,10.16,10.87,11.59,7.32,9.88,8.67,10.33,8.55,9.36,10.61,9.54,8.67,10.41,10.90,11.16,9.34,9.51,6.54,9.89,7.74,8.94,10.82
],
    'Feature8': [ #인구(천 명)
        84.4,643.2,297.1,369.7,548.8,404.5,220.3,398,214.1,423.4,230.2,324.2,361,500,284,368.6,546,319,369,338,85.7,309.4,197,220,353
], 
    'Target': [
    3269,21201,7398,30441,8078,16753,4557,7271,4332,33000,28160,5218,1884,52347,1477,8826,1918,6569,4789,21578,5467,7232,2631,3556,345
    ]
})

# 독립 변수와 종속 변수 분리
X = data.drop(columns=['Target'])
y = data['Target']

# 1. 상관 행렬 계산
correlation_matrix = X.copy()
correlation_matrix['Target'] = y
correlation_matrix = correlation_matrix.corr()

# 2. 상관 행렬 히트맵 시각화
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)
plt.title('Correlation Matrix of Features and Target')
plt.show()

# 3. 각 Feature와 Target 간의 산점도 플롯
plt.figure(figsize=(15, 12))
for i, feature in enumerate(X.columns, 1):
    plt.subplot(3, 3, i)  # 3x3 그리드로 산점도 플롯 배치
    plt.scatter(X[feature], y, alpha=0.7)
    plt.xlabel(feature)
    plt.ylabel('Target')
    plt.title(f'Scatter Plot of {feature} vs Target')
plt.tight_layout()
plt.show()

# 4. 데이터 표준화
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5. PCA 수행
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# 6. 각 주성분의 분산 비율
explained_variance_ratio = pca.explained_variance_ratio_

# 7. 주성분의 누적 분산 비율
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

# 8. PCA 결과 시각화
plt.figure(figsize=(12, 8))
plt.plot(range(1, len(explained_variance_ratio) + 1), cumulative_variance_ratio, marker='o', linestyle='--')
plt.axhline(y=0.9, color='r', linestyle='--')  # 90% 기준선
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.title('PCA Explained Variance Ratio')
plt.grid()
plt.show()

# 9. PCA의 주요 변수 시각화
pca_components = pd.DataFrame(pca.components_, columns=X.columns, index=[f'PC{i+1}' for i in range(X.shape[1])])
plt.figure(figsize=(12, 8))
sns.heatmap(pca_components, cmap='coolwarm', annot=True)
plt.title('PCA Components')
plt.show()

# 10. 주성분 분석 결과 출력
print("각 주성분의 분산 비율:")
for i, variance in enumerate(explained_variance_ratio, start=1):
    print(f"Principal Component {i}: {variance:.4f}")

print("\n누적 분산 비율:")
for i, cumulative_variance in enumerate(cumulative_variance_ratio, start=1):
    print(f"Up to Principal Component {i}: {cumulative_variance:.4f}")
